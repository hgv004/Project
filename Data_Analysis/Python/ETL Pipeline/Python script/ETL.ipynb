{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e62935f3-dae5-44f3-8a57-9da76eda7402",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ebea8fdc-7339-4526-90ae-3d516adcdee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.21)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy) (2.0.2)\n",
      "Requirement already satisfied: pymysql in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy\n",
    "!pip install pymysql\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f612a063-d65c-4290-b3fd-df01e2d7a0a7",
   "metadata": {},
   "source": [
    "# MYSQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f96ead8-0c9d-40d6-9fa7-17e49e8840fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = r'mysql+pymysql://root:root@localhost:3306/etl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8454ea-622e-4f0d-ae4d-7316ae65bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(db_url)\n",
    "mydb = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae7976-4901-46b7-8124-84bba6939249",
   "metadata": {},
   "source": [
    "## Write First file to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c554bdc3-42e1-43de-8bcc-ec22eb5a4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_local = pd.read_csv(r\"C:\\Hardik\\Projects\\Data_Analysis\\Python\\ETL Pipeline\\Datasource\\f1_local\\2023-09-15.csv\")\n",
    "f2_local = pd.read_csv(r\"C:\\Hardik\\Projects\\Data_Analysis\\Python\\ETL Pipeline\\Datasource\\f2_local\\2023-09-15.csv\")\n",
    "f3_local = pd.read_csv(r\"C:\\Hardik\\Projects\\Data_Analysis\\Python\\ETL Pipeline\\Datasource\\f3_local\\2023-09-15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31ca6d93-db41-4655-a1a8-5275619e324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_local['updated_date'] = datetime.strptime('2023-09-15', \"%Y-%m-%d\").date()\n",
    "f2_local['updated_date'] = datetime.strptime('2023-09-15', \"%Y-%m-%d\").date()\n",
    "f3_local['updated_date'] = datetime.strptime('2023-09-15', \"%Y-%m-%d\").date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40789d4d-0245-45a0-b0d6-7bfb214e7305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_local.to_sql(name='f1', con=engine, if_exists=\"replace\", index=False)\n",
    "f2_local.to_sql(name='f2', con=engine, if_exists=\"replace\", index=False)\n",
    "f3_local.to_sql(name='f3', con=engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb38281-fbd0-4f21-8ddb-ff7f84a2d128",
   "metadata": {},
   "source": [
    "# Prepare Function which appends all the tables from local, which is not present in database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5b8537ea-bdd6-489e-a0d0-3e9672d095c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict = {}\n",
    "fpath = f\"C:\\Hardik\\Projects\\Data_Analysis\\Python\\ETL Pipeline\\Datasource\"\n",
    "\n",
    "# Prepare function to combine sftp files dataframe\n",
    "def comb_data(folder, db_table):\n",
    "    Local_filelist_csv = os.listdir(fpath + f\"/\" + folder)\n",
    "    # print(Local_filelist_csv)\n",
    "    Local_filelist = pd.Series(Local_filelist_csv).str[:-4]\n",
    "    # print(Local_filelist)\n",
    "    \n",
    "    Local_filelist_date = pd.to_datetime(Local_filelist, format=\"%Y-%m-%d\").dt.date\n",
    "    # print(Local_filelist_date)\n",
    "    \n",
    "    # Extract file with latest date\n",
    "    maxdate_local = datetime.strptime(max(Local_filelist), \"%Y-%m-%d\")\n",
    "    maxdate_local = maxdate_local.date()\n",
    "    # print('maxdate_local---->>', maxdate_local)\n",
    "    \n",
    "      \n",
    "    # Query \n",
    "    q = text(f\"SELECT distinct updated_date FROM {db_table}\")\n",
    "    q1 = text(f\"SELECT max(updated_date) FROM {db_table}\")\n",
    "    # print(q)\n",
    "    \n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(q1)\n",
    "        latest_date_db = result.fetchall()[0][0]\n",
    "        # print('latest_date_db------>>',latest_date_db)\n",
    "\n",
    "    files_to_append = []\n",
    "    for i in Local_filelist_date:\n",
    "        if i > latest_date_db:\n",
    "            files_to_append.append(i.strftime(\"%Y-%m-%d\") + \".csv\")\n",
    "    # print('files_to_append----->',files_to_append)\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    for i in files_to_append:\n",
    "        local_filepath = fpath + \"\\\\\" + folder + \"\\\\\" + i\n",
    "        # print(local_filepath)\n",
    "        df1 = pd.read_csv(local_filepath, index_col=False)\n",
    "        df1[\"updated_date\"] = i[:-4]\n",
    "        data = pd.concat([data, df1], ignore_index=True)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b54eaee-28d0-43fc-b5dd-90fe4604a8df",
   "metadata": {},
   "source": [
    "# Write New Files to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b316a947-51e7-4761-a85e-d295f6a33811",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_files = {\"f1\": \"f1_local\", \"f2\": \"f2_local\", \"f3\": \"f3_local\"}\n",
    "\n",
    "# Insert dataframe into the datadict using function\n",
    "for db_table, local_folder in tables_files.items():\n",
    "    datadict[db_table] = comb_data(local_folder, db_table)\n",
    "\n",
    "# Write Dataframes into Database\n",
    "for db_table, df_to_append in datadict.items():\n",
    "    df_to_append.to_sql(name=db_table, con=engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f09dc5-138d-4e8b-a59a-bc26d01948d9",
   "metadata": {},
   "source": [
    "# Final Code in Single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6506557-0f6a-4691-9cc3-bf80652a8e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.21)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy) (2.0.2)\n",
      "Requirement already satisfied: pymysql in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install sqlalchemy\n",
    "# !pip install pymysql\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "from datetime import datetime\n",
    "\n",
    "db_url = r'mysql+pymysql://root:root@localhost:3306/etl'\n",
    "\n",
    "engine = create_engine(db_url)\n",
    "mydb = engine.connect()\n",
    "\n",
    "datadict = {}\n",
    "fpath = f\"C:\\Hardik\\Projects\\Data_Analysis\\Python\\ETL Pipeline\\Datasource\"\n",
    "\n",
    "# Prepare function to combine sftp files dataframe\n",
    "def comb_data(folder, db_table):\n",
    "    Local_filelist_csv = os.listdir(fpath + f\"/\" + folder)\n",
    "    # print(Local_filelist_csv)\n",
    "    Local_filelist = pd.Series(Local_filelist_csv).str[:-4]\n",
    "    # print(Local_filelist)\n",
    "    \n",
    "    Local_filelist_date = pd.to_datetime(Local_filelist, format=\"%Y-%m-%d\").dt.date\n",
    "    # print(Local_filelist_date)\n",
    "    \n",
    "    # Extract file with latest date\n",
    "    maxdate_local = datetime.strptime(max(Local_filelist), \"%Y-%m-%d\")\n",
    "    maxdate_local = maxdate_local.date()\n",
    "    # print('maxdate_local---->>', maxdate_local)\n",
    "    \n",
    "      \n",
    "    # Query \n",
    "    q = text(f\"SELECT distinct updated_date FROM {db_table}\")\n",
    "    q1 = text(f\"SELECT max(updated_date) FROM {db_table}\")\n",
    "    # print(q)\n",
    "    \n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(q1)\n",
    "        latest_date_db = result.fetchall()[0][0]\n",
    "        # print('latest_date_db------>>',latest_date_db)\n",
    "\n",
    "    files_to_append = []\n",
    "    for i in Local_filelist_date:\n",
    "        if i > latest_date_db:\n",
    "            files_to_append.append(i.strftime(\"%Y-%m-%d\") + \".csv\")\n",
    "    # print('files_to_append----->',files_to_append)\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    for i in files_to_append:\n",
    "        local_filepath = fpath + \"\\\\\" + folder + \"\\\\\" + i\n",
    "        # print(local_filepath)\n",
    "        df1 = pd.read_csv(local_filepath, index_col=False)\n",
    "        df1[\"updated_date\"] = i[:-4]\n",
    "        data = pd.concat([data, df1], ignore_index=True)\n",
    "    return data\n",
    "    \n",
    "tables_files = {\"f1\": \"f1_local\", \"f2\": \"f2_local\", \"f3\": \"f3_local\"}\n",
    "\n",
    "# Insert dataframe into the datadict using function\n",
    "for db_table, local_folder in tables_files.items():\n",
    "    datadict[db_table] = comb_data(local_folder, db_table)\n",
    "\n",
    "# Write Dataframes into Database\n",
    "for db_table, df_to_append in datadict.items():\n",
    "    df_to_append.to_sql(name=db_table, con=engine, if_exists=\"append\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
